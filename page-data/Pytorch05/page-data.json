{"componentChunkName":"component---src-templates-post-tsx","path":"/Pytorch05/","result":{"data":{"markdownRemark":{"html":"<h2 id=\"희소-표현sparce-representation\" style=\"position:relative;\"><a href=\"#%ED%9D%AC%EC%86%8C-%ED%91%9C%ED%98%84sparce-representation\" aria-label=\"희소 표현sparce representation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>희소 표현(Sparce Representation)</h2>\n<p>희소 표현이란 데이터가 희소하다는 의미입니다. 여기서 데이터란 '우리가 관심을 갖는, 의미있는 정보'정도로 해석할 수 있을 것 같습니다.</p>\n<p>이해하기 쉽게 예를 들어 설명해봅시다.</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">[0, 1, 0, 0, 0, 0, 0, 0, 0]\n[0, 0, 0, 1, 0, 0, 0, 0, 0]\n[0, 0, 0, 0, 0, 0, 1, 0, 0]</code></pre></div>\n<p>위의 벡터들은 희소 벡터라고 할 수 있습니다. 우리가 알고싶어 하는 정보의 인덱스만 1로 표현되고, 나머지 원소들은 0으로 이루어진 벡터입니다. 따라서 실제 표현하는 데이터의 양은 희소(Sparse)하며, 데이터를 표현하지 않는 나머지는 모두 0으로 표현됩니다.</p>\n<p>이와 같이 표현하는 것을 희소 표현이라고 합니다.</p>\n<p>지난 <a href=\"https://mintyu.github.io/Pytorch04/\" target=\"_blank\" rel=\"nofollow noopener noreferrer\">원-핫 인코딩</a> 게시물에서 배운 원-핫 인코딩을 통해 나온 원-핫 벡터 또한 희소 벡터입니다.</p>\n<h2 id=\"밀집-표현dense-representation\" style=\"position:relative;\"><a href=\"#%EB%B0%80%EC%A7%91-%ED%91%9C%ED%98%84dense-representation\" aria-label=\"밀집 표현dense representation permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>밀집 표현(Dense Representation)</h2>\n<p>밀집 표현(Dense Representation)은 희소 표현(Sparse Representation)과 반대대는 표현입니다. 희소 표현의 일종인 원-핫 인코딩에서는 벡터의 차원이 곧 단어 집합의 크기이고, 표현하고자 하는 단어의 인덱스의 값만 1이고 나머지는 모두 0인 <code class=\"language-text\">ex) [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]</code> 표현 방식이지만, 밀집 표현은 다릅니다.</p>\n<p>만약 단어 집합의 크기가 10000이라 가정해본다면, 이를 원-핫 인코딩을 통해 희소 벡터(희소 표현을 통해 생성된 벡터)로 표현하게 되면 차원이 10000인 희소 벡터가 생성될 것입니다.</p>\n<p>하지만 이를 밀집 표현으로 나타내고 사용자가 밀집 표현의 차원을 128로 설정했다면, 밀집 벡터(밀집 표현을 통해 생성된 벡터)의 차원은 128이 되며 벡터 안의 모든 값들은 실수가 됩니다.</p>\n<p>이와 같이 단어를 <strong>밀집 벡터</strong>로 표현하는 과정을 <strong>워드 임베딩</strong>이라고 합니다.</p>\n<h2 id=\"워드-임베딩word-embedding\" style=\"position:relative;\"><a href=\"#%EC%9B%8C%EB%93%9C-%EC%9E%84%EB%B2%A0%EB%94%A9word-embedding\" aria-label=\"워드 임베딩word embedding permalink\" class=\"anchor before\"><svg aria-hidden=\"true\" focusable=\"false\" height=\"16\" version=\"1.1\" viewBox=\"0 0 16 16\" width=\"16\"><path fill-rule=\"evenodd\" d=\"M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z\"></path></svg></a>워드 임베딩(Word Embedding)</h2>\n<p>워드 임베딩이란, 단어를 <strong>밀집 벡터(Dense Vector)</strong>의 형태로 표현하는 방법입니다. 또한 이 워드 임베딩 과정을 거쳐 나온 밀집 벡터의 이름을 <strong>임베딩 벡터(Embedding Vector)</strong>라고 합니다.</p>\n<p>아래는 원-핫 벡터와 임베딩 벡터의 차이를 나타낸 표입니다.</p>\n<table>\n<thead>\n<tr>\n<th></th>\n<th>원-핫 벡터</th>\n<th>임베딩 벡터</th>\n</tr>\n</thead>\n<tbody>\n<tr>\n<td>차원</td>\n<td>고차원</td>\n<td>저차원</td>\n</tr>\n<tr>\n<td>다른 표현</td>\n<td>희소 벡터의 일종</td>\n<td>밀집 벡터의 일종</td>\n</tr>\n<tr>\n<td>표현 방법</td>\n<td>수동</td>\n<td>훈련 데이터로부터 학습</td>\n</tr>\n<tr>\n<td>값의 타입</td>\n<td>0, 1</td>\n<td>real number</td>\n</tr>\n</tbody>\n</table>","excerpt":"희소 표현(Sparce Representation…","tableOfContents":"<ul>\n<li><a href=\"/Pytorch05/#%ED%9D%AC%EC%86%8C-%ED%91%9C%ED%98%84sparce-representation\">희소 표현(Sparce Representation)</a></li>\n<li><a href=\"/Pytorch05/#%EB%B0%80%EC%A7%91-%ED%91%9C%ED%98%84dense-representation\">밀집 표현(Dense Representation)</a></li>\n<li><a href=\"/Pytorch05/#%EC%9B%8C%EB%93%9C-%EC%9E%84%EB%B2%A0%EB%94%A9word-embedding\">워드 임베딩(Word Embedding)</a></li>\n</ul>","fields":{"slug":"/Pytorch05/"},"frontmatter":{"title":"자연어 처리(NLP)의 전처리 - 워드 임베딩","date":"May 02, 2021","tags":["Pytorch"],"keywords":["Pytorch","Deep Learning","Machine Learning","NLP","Natural Language Processing","머신러닝","딥러닝","자연어 처리","워드 임베딩","Word Embedding"],"update":"Jan 01, 0001"}}},"pageContext":{"slug":"/Pytorch05/","series":[],"lastmod":"0001-01-01"}},"staticQueryHashes":["3649515864","63159454"]}