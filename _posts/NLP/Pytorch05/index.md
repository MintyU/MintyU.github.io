---
title: 자연어 처리(NLP)의 전처리 - 워드 임베딩
date: 2021-05-02
tags:
  - Pytorch
keywords:
  - Pytorch
  - Deep Learning
  - Machine Learning
  - NLP
  - Natural Language Processing
  - 머신러닝
  - 딥러닝
  - 자연어 처리
  - 워드 임베딩
  - Word Embedding
---

## 희소 표현(Sparce Representation)

희소 표현이란 데이터가 희소하다는 의미입니다. 여기서 데이터란 '우리가 관심을 갖는, 의미있는 정보'정도로 해석할 수 있을 것 같습니다.

이해하기 쉽게 예를 들어 설명해봅시다.

```
[1, 0, 0, 0, 0, 0, 0, 0, 0]
[0, 1, 0, 0, 0, 0, 0, 0, 0]
[0, 0, 1, 0, 0, 0, 0, 0, 0]

...

```
위의 벡터들은 희소 벡터라고 할 수 있습니다. 우리가 알고싶어 하는 정보의 인덱스만 1로 표현되고, 나머지 원소들은 0으로 이루어진 벡터입니다. 따라서 실제 표현하는 데이터의 양은 희소(Sparse)하며, 데이터를 표현하지 않는 나머지는 모두 0으로 표현됩니다.

이와 같이 표현하는 것을 희소 표현이라고 합니다.

지난 [원-핫 인코딩](https://mintyu.github.io/Pytorch04/) 게시물에서 배운 원-핫 인코딩을 통해 나온 원-핫 벡터 또한 희소 벡터입니다.

## 밀집 표현(Dense Representation)

밀집 표현(Dense Representation)은 희소 표현(Sparse Representation)과 반대대는 표현입니다. 희소 표현의 일종인 원-핫 인코딩에서는 벡터의 차원이 곧 단어 집합의 크기이고, 표현하고자 하는 단어의 인덱스의 값만 1이고 나머지는 모두 0인 `ex) [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]` 표현 방식이지만, 밀집 표현은 다릅니다.

만약 단어 집합의 크기가 10000이라 가정해본다면, 이를 원-핫 인코딩을 통해 희소 벡터(희소 표현을 통해 생성된 벡터)로 표현하게 되면 차원이 10000인 희소 벡터가 생성될 것입니다.

하지만 이를 밀집 표현으로 나타내고 사용자가 밀집 표현의 차원을 128로 설정했다면, 밀집 벡터(밀집 표현을 통해 생성된 벡터)의 차원은 128이 되며 벡터 안의 모든 값들은 실수가 됩니다.

이와 같이 단어를 **밀집 벡터**로 표현하는 과정을 **워드 임베딩**이라고 합니다.


## 워드 임베딩(Word Embedding)

워드 임베딩이란, 단어를 **밀집 벡터(Dense Vector)**의 형태로 표현하는 방법입니다. 또한 이 워드 임베딩 과정을 거쳐 나온 밀집 벡터의 이름을 **임베딩 벡터(Embedding Vector)**라고 합니다.

아래는 원-핫 벡터와 임베딩 벡터의 차이를 나타낸 표입니다.

| |원-핫 벡터|임베딩 벡터|
|------|---|---|
|차원|고차원|저차원|
|다른 표현|희소 벡터의 일종|밀집 벡터의 일종|
|표현 방법|수동|훈련 데이터로부터 학습|
|값의 타입|0, 1|real number|
